{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12130955,"sourceType":"datasetVersion","datasetId":7639182},{"sourceId":12150042,"sourceType":"datasetVersion","datasetId":7652139}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers accelerate bitsandbytes pandas Pillow tqdm scipy safetensors","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom PIL import Image\nfrom transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = InstructBlipForConditionalGeneration.from_pretrained(\n                \"Salesforce/instructblip-vicuna-7b\",\n                torch_dtype=torch.float16,\n                load_in_8bit=True,\n                device_map=\"auto\")\nprocessor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\", \n                                                  use_fast=True) #use 13b for error response\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load CSV\n\"\"\"\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"The `language_model` is not in the `hf_device_map`\")\n\ndf = pd.read_csv('/kaggle/input/base-caption-lack/base_caption.csv')\n\n# Config paths\nIMAGE_DIR = \"/kaggle/input/track-1-private-set/Track 1 - Private Set/matched_images/\"  # Adjust path\nIMAGE_EXT = \".jpg\"  # or \".png\", \".jpeg\"\n\nempty_caption_mask = df['caption'].isna() | (df['caption'] == '')\nempty_indices = df[empty_caption_mask].index\n\n# Process images\n\nresults = []\nfor idx in empty_indices:\n    try:\n        # Load image\n        image_path = f\"{IMAGE_DIR}{df.loc[idx, 'matched_image_id']}{IMAGE_EXT}\"\n        image = Image.open(image_path)\n        \n        # Generate caption\n        prompt = \"Describe this image in detail. Focus on the visible people, objects, setting, activities, lighting, atmosphere and any notable elements that would be important for news reporting. Focus on factual details and spatial relationships to create a rich visual narrative.\"\n        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n        \n        with torch.no_grad():\n            outputs = model.generate(**inputs, \n                                     max_length = 350, \n                                     min_length = 50, \n                                     length_penalty=1.2,\n                                     repetition_penalty=1.12,\n                                     num_beams=5,\n                                     pad_token_id=processor.tokenizer.eos_token_id,\n                                     early_stopping=True)\n        \n        caption_full = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n        if prompt in caption_full:\n            caption = caption_full.split(prompt)[1].strip()\n        else:\n            caption = caption_full\n        del inputs, outputs\n        torch.cuda.empty_cache()\n        gc.collect()\n        df.loc[idx, 'caption'] = caption\n        print(f\"Processed {df.loc[idx, 'query_id']}\")\n        \n    except Exception as e:\n        print(f\"Error processing {df.loc[idx, 'query_id']}: {e}\")\n\ndf.to_csv(\"updated_caption.csv\", index=False)\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load CSV\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"The `language_model` is not in the `hf_device_map`\")\n\ndf = pd.read_csv('/kaggle/input/matched-images-real/link_images.csv') #Adjust your path, link_images.csv is the file csv that contains the query_id and the matched image in database\n\n# Config paths\nIMAGE_DIR = \"/kaggle/input/track-1-private-set/Track 1 - Private Set/matched_images/\"  # Adjust path (image database folder)\nIMAGE_EXT = \".jpg\"  # or \".png\", \".jpeg\"\n\n# Choose which half to process\nPROCESS_HALF = \"first\"  # \"first\" or \"second\"\n\nif PROCESS_HALF == \"first\":\n    df_subset = df[:len(df)//2]\n    print(f\"Processing first half: {len(df_subset)} rows\")\nelif PROCESS_HALF == \"second\": \n    df_subset = df[len(df)//2:]\n    print(f\"Processing second half: {len(df_subset)} rows\")\n\n# Process images\nresults = []\nfor _, row in df_subset.head(2).iterrows():\n    try:\n        # Load image\n        image_path = f\"{IMAGE_DIR}{row['matched_image_id']}{IMAGE_EXT}\"\n        image = Image.open(image_path)\n        \n        # Generate caption\n        prompt = \"Describe this image in detail. Focus on the visible people, objects, setting, activities, lighting, atmosphere and any notable elements that would be important for news reporting. Focus on factual details and spatial relationships to create a rich visual narrative.\"\n        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n        \n        with torch.no_grad():\n            outputs = model.generate(**inputs, \n                                     max_length = 350, \n                                     min_length = 50, \n                                     length_penalty=1.0,\n                                     repetition_penalty=1.1,\n                                     num_beams=5,\n                                     pad_token_id=processor.tokenizer.eos_token_id,\n                                     early_stopping=True)\n        \n        caption_full = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n        caption_full = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n        if prompt in caption_full:\n            caption = caption_full.split(prompt)[1].strip()\n        else:\n            caption = caption_full\n        del inputs, outputs\n        torch.cuda.empty_cache()\n        gc.collect()\n        results.append({\n            'query_id': row['query_id'],\n            'matched_image_id': row['matched_image_id'],\n            'caption': caption\n        })\n        \n        print(f\"Processed {row['query_id']}\")\n        \n    except Exception as e:\n        print(f\"Error processing {row['query_id']}: {e}\")\n\n# Save results\nresults_df = pd.DataFrame(results)\nresults_df.to_csv('captions.csv', index=False)\nprint(f\"Generated {len(results)} captions\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}