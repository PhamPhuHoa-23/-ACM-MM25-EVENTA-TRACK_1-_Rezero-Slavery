{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installation"
      ],
      "metadata": {
        "id": "ZKZsew-40YDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install transformers==4.51.3\n",
        "    !pip install --no-deps unsloth"
      ],
      "metadata": {
        "id": "9lgpzUZk0YDH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-06T03:18:45.764707Z",
          "iopub.execute_input": "2025-06-06T03:18:45.765050Z",
          "iopub.status.idle": "2025-06-06T03:19:04.617415Z",
          "shell.execute_reply.started": "2025-06-06T03:18:45.765021Z",
          "shell.execute_reply": "2025-06-06T03:19:04.616335Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers numpy"
      ],
      "metadata": {
        "id": "iXwTBNwQwHQk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-06T03:22:43.575325Z",
          "iopub.execute_input": "2025-06-06T03:22:43.575641Z",
          "iopub.status.idle": "2025-06-06T03:24:07.049306Z",
          "shell.execute_reply.started": "2025-06-06T03:22:43.575618Z",
          "shell.execute_reply": "2025-06-06T03:24:07.048353Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "fsFFWS_7WN7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cấu hình model\n",
        "MODEL_CONFIG = {\n",
        "    \"model_name\": \"unsloth/DeepSeek-R1-0528-Qwen3-8B\",\n",
        "    \"max_seq_length\": 12000,\n",
        "    \"load_in_4bit\": True,\n",
        "    \"load_in_8bit\": False,\n",
        "    \"full_finetuning\": False,\n",
        "}"
      ],
      "metadata": {
        "id": "vyKt7F7-Khty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cấu hình LoRA\n",
        "LORA_CONFIG = {\n",
        "    \"r\": 256,\n",
        "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    \"lora_alpha\": 256,\n",
        "    \"lora_dropout\": 0,\n",
        "    \"bias\": \"none\",\n",
        "    \"use_gradient_checkpointing\": \"unsloth\",\n",
        "    \"random_state\": 3407,\n",
        "    \"use_rslora\": False,\n",
        "    \"loftq_config\": None,\n",
        "}\n"
      ],
      "metadata": {
        "id": "HqbfDdYWKopd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cấu hình training\n",
        "TRAINING_CONFIG = {\n",
        "    \"output_dir\": \"./sft_results_upgraded_prompt\",\n",
        "    \"dataset_text_field\": \"text\",\n",
        "    \"max_seq_length\": 12000,\n",
        "    \"per_device_train_batch_size\": 8,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"warmup_steps\": 10,\n",
        "    \"num_train_epochs\": 2,\n",
        "    \"max_steps\": -1,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"logging_steps\": 1,\n",
        "    \"optim\": \"paged_adamw_8bit\",\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"lr_scheduler_type\": \"cosine\",\n",
        "    \"seed\": 3407,\n",
        "    \"report_to\": \"none\",\n",
        "    \"dataloader_num_workers\": 3,\n",
        "    \"dataset_num_proc\": 1,\n",
        "}"
      ],
      "metadata": {
        "id": "4imsiHEtKxGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ngưỡng xử lý content\n",
        "CONTENT_LENGTH_THRESHOLD_FOR_SUMMARY = 1000\n",
        "MAX_SUMMARY_LEN_IN_PROMPT = 6000\n",
        "MAX_LEN_SEMANTIC_EXTRACTION = 6000\n",
        "\n",
        "\n",
        "NUM_PROC = 1\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/\"\n",
        "\n",
        "DATASET_PATH = \"output_merged.json\"\n"
      ],
      "metadata": {
        "id": "A-ZvOHT1KyTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "USE_SEMANTIC_SEARCH = True\n",
        "MODEL_NAME_SEMANTIC = 'all-MiniLM-L12-v2'\n",
        "sentence_model_global = None\n",
        "\n",
        "if USE_SEMANTIC_SEARCH:\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        sentence_model_global = SentenceTransformer(MODEL_NAME_SEMANTIC)\n",
        "    except ImportError:\n",
        "        USE_SEMANTIC_SEARCH = False\n",
        "    except Exception as e:\n",
        "        USE_SEMANTIC_SEARCH = False\n"
      ],
      "metadata": {
        "id": "9jxbmLasARAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1F0EMvGBm-l4iXV11Im3zaGOPbifsWxfQ #Link of training dataset for caption generation stage"
      ],
      "metadata": {
        "id": "t6Ri6khaiVCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def _identify_topic(text: str, titles: list) -> str:\n",
        "    all_text = ' '.join(titles).lower() + ' ' + text.lower()\n",
        "    topic_keywords = {\n",
        "        'technology': ['technology', 'tech', 'software', 'AI', 'robot', 'digital', 'computer', 'innovation', 'platform', 'data'],\n",
        "        'business': ['business', 'company', 'market', 'economy', 'trade', 'finance', 'investment', 'ceo', 'gdp', 'stock', 'enterprise'],\n",
        "        'politics': ['election', 'president', 'government', 'policy', 'political', 'minister', 'congress', 'parliament', 'senate', 'legislation', 'bill', 'campaign'],\n",
        "        'sports': ['game', 'player', 'team', 'match', 'championship', 'athlete', 'sport', 'win', 'tournament', 'olympics', 'score'],\n",
        "        'health': ['health', 'medical', 'doctor', 'patient', 'disease', 'treatment', 'hospital', 'vaccine', 'pandemic', 'healthcare', 'medicine'],\n",
        "        'environment': ['climate', 'environment', 'pollution', 'renewable', 'energy', 'sustainable', 'carbon', 'green', 'emissions', 'ecology'],\n",
        "        'entertainment': ['movie', 'film', 'actor', 'music', 'artist', 'show', 'entertainment', 'celebrity', 'concert', 'awards'],\n",
        "        'science': ['research', 'study', 'scientist', 'discovery', 'experiment', 'science', 'data', 'analysis', 'journal', 'university'],\n",
        "        'education': ['education', 'school', 'university', 'college', 'student', 'teacher', 'learning', 'curriculum'],\n",
        "        'social issues': ['social', 'community', 'human rights', 'inequality', 'poverty', 'justice', 'protest'],\n",
        "        'world affairs': ['international', 'global', 'world', 'geopolitics', 'diplomacy', 'conflict', 'united nations'],\n",
        "        'food': ['food', 'restaurant', 'chef', 'meal', 'cuisine', 'cooking', 'recipe', 'dining', 'agriculture'],\n",
        "        'travel': ['travel', 'tourism', 'destination', 'flight', 'hotel', 'vacation', 'journey', 'trip', 'airport']\n",
        "    }\n",
        "    topic_scores = {}\n",
        "    for topic, keywords in topic_keywords.items():\n",
        "        score = sum(1 for keyword in keywords if keyword in all_text)\n",
        "        if score > 0:\n",
        "            topic_scores[topic] = score\n",
        "    if topic_scores:\n",
        "        return max(topic_scores, key=topic_scores.get)\n",
        "    return 'general'\n",
        "\n",
        "def _extract_organizations(text: str) -> list:\n",
        "    organizations = []\n",
        "\n",
        "    patterns = [\n",
        "        r'\\b[A-Z]{2,6}\\b',\n",
        "        r'\\b[A-Z][a-zA-Z]*(?:\\s+(?:and|of|the|for)\\s+)?[A-Z][a-zA-Z]*(?:\\s+(?:Inc|Corp|Ltd|LLC|Co|Group|Holdings|Foundation|Association|Organization|Agency|Department|University|Institute|College|School|Council|Committee|Party|Union|Bank|Studio|Network))\\b',\n",
        "        r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\s+(?:Inc\\.|Corp\\.|Ltd\\.|LLC|Co\\.|Group|Foundation|Association|Organization|Agency|Department|University|Institute|College|School|Council|Committee|Party|Union|Bank|Studio|Network)\\b',\n",
        "    ]\n",
        "\n",
        "    known_orgs = [\n",
        "        'Google', 'Microsoft', 'Apple', 'Amazon', 'Meta', 'Facebook', 'Twitter', 'Netflix', 'Tesla',\n",
        "        'United Nations', 'World Health Organization', 'European Union', 'NATO', 'NASA',\n",
        "        'CNN', 'BBC', 'Reuters', 'Associated Press', 'New York Times', 'The Guardian',\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        organizations.extend(matches)\n",
        "\n",
        "    for org in known_orgs:\n",
        "        if re.search(r'\\b' + re.escape(org) + r'\\b', text, re.IGNORECASE):\n",
        "            organizations.append(org)\n",
        "\n",
        "\n",
        "    processed_orgs = []\n",
        "    common_words_in_org_names = {'The', 'A', 'An', 'Of', 'And', 'For'}\n",
        "    for org in organizations:\n",
        "        org_stripped = org.strip()\n",
        "\n",
        "        if len(org_stripped) <= 1 and org_stripped.isupper():\n",
        "            continue\n",
        "        if org_stripped.isupper() and len(org_stripped) > 6:\n",
        "             if org_stripped not in known_orgs:\n",
        "                continue\n",
        "\n",
        "        if org_stripped in common_words_in_org_names:\n",
        "            continue\n",
        "        processed_orgs.append(org_stripped)\n",
        "\n",
        "\n",
        "    final_orgs = list(set(processed_orgs))\n",
        "    final_orgs.sort(key=lambda x: (len(x.split()), x.isupper()), reverse=True)\n",
        "    return final_orgs[:8]\n",
        "\n",
        "\n",
        "def _extract_people(text: str) -> list:\n",
        "    people = []\n",
        "\n",
        "    patterns = [\n",
        "        r'\\b(?:Mr\\.|Mrs\\.|Ms\\.|Miss|Dr\\.|Prof\\.|President|CEO|Minister|Director|Ambassador|General|Captain|Chef|Senator|Governor|Mayor|Councillor|Judge|Justice)\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){0,2})\\b',\n",
        "\n",
        "        r'\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z\\'\\-]+){1,3})\\b'\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "\n",
        "        if isinstance(matches, list) and matches and isinstance(matches[0], tuple):\n",
        "             people.extend([m[0] for m in matches if m[0]])\n",
        "        else:\n",
        "             people.extend(matches)\n",
        "\n",
        "    non_name_keywords = {\n",
        "        'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December',\n",
        "        'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday',\n",
        "        'Today', 'Yesterday', 'Tomorrow', 'Week', 'Month', 'Year',\n",
        "        'Street', 'Road', 'Avenue', 'City', 'State', 'Country', 'County', 'Park', 'Building', 'Center', 'Plaza', 'Square',\n",
        "        'The', 'This', 'That', 'These', 'Those', 'And', 'But', 'For', 'With', 'From', 'About', 'Under', 'Over',\n",
        "        'Is', 'Are', 'Was', 'Were', 'Has', 'Have', 'Had', 'Says', 'Said', 'Told',\n",
        "        'North', 'South', 'East', 'West',\n",
        "        'Company', 'Corporation', 'Incorporated', 'Limited', 'Organization', 'Department', 'University', 'Institute', 'College', 'School' # Tên tổ chức\n",
        "    }\n",
        "\n",
        "    processed_people = []\n",
        "    for p_match in people:\n",
        "        p = p_match.strip()\n",
        "        words = p.split()\n",
        "\n",
        "        if len(words) >= 2 and all(word[0].isupper() for word in words) and not all(word.isupper() for word in words) and not any(word in non_name_keywords for word in words) and len(p)>3 :\n",
        "            processed_people.append(p)\n",
        "        elif len(words) == 1 and p[0].isupper() and p not in non_name_keywords and len(p)>3 and not p.isupper(): # Tên một từ\n",
        "\n",
        "            processed_people.append(p)\n",
        "\n",
        "\n",
        "\n",
        "    final_people = list(set(processed_people))\n",
        "    final_people.sort(key=len, reverse=True)\n",
        "    return final_people[:8]\n",
        "\n",
        "def _extract_locations(text: str) -> list:\n",
        "    locations = []\n",
        "\n",
        "    predefined_locations = [\n",
        "        'Vietnam', 'United States', 'China', 'India', 'Japan', 'Germany', 'United Kingdom', 'France', 'Canada', 'Australia', 'Russia', 'Brazil', 'South Korea', 'Italy', 'Spain',\n",
        "        'New York', 'Los Angeles', 'Chicago', 'London', 'Paris', 'Berlin', 'Tokyo', 'Beijing', 'Shanghai', 'Seoul', 'Moscow', 'Singapore', 'Sydney', 'Toronto', 'Rome', 'Madrid', 'Washington D.C.'\n",
        "        'San Francisco', 'Silicon Valley'\n",
        "    ]\n",
        "    for loc in predefined_locations:\n",
        "        if re.search(r'\\b' + re.escape(loc) + r'\\b', text, re.IGNORECASE):\n",
        "            locations.append(loc)\n",
        "\n",
        "    patterns = [\n",
        "        r'\\b([A-Z][a-zA-Z\\']+)(?:\\s+(?:of|de|the|la)\\s+)?(?:[A-Z][a-zA-Z\\']+){0,3}(?:,\\s*[A-Z][a-zA-Z\\.\\s]+)?\\b'\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        locations.extend(match.strip() for match in matches if len(match.strip()) > 2) # Lọc kết quả\n",
        "\n",
        "\n",
        "    non_location_keywords = _extract_people(text) + _extract_organizations(text) + [\n",
        "        'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December',\n",
        "        'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'Mr', 'Ms', 'Dr'\n",
        "    ]\n",
        "    non_location_keywords_lower = {k.lower() for k in non_location_keywords}\n",
        "\n",
        "    processed_locations = []\n",
        "    for loc in locations:\n",
        "        loc_stripped = loc.strip().rstrip(',.')\n",
        "\n",
        "        if len(loc_stripped.split()) == 1 and (loc_stripped.isdigit() or (loc_stripped.isupper() and loc_stripped not in ['US', 'UK', 'EU'])):\n",
        "            continue\n",
        "        if loc_stripped.lower() not in non_location_keywords_lower and len(loc_stripped)>2:\n",
        "            if not (loc_stripped.lower().startswith(\"the \") and len(loc_stripped.split()) < 3):\n",
        "                 processed_locations.append(loc_stripped)\n",
        "\n",
        "\n",
        "    final_locations = list(set(processed_locations))\n",
        "    final_locations.sort(key=len, reverse=True)\n",
        "    return final_locations[:8]\n",
        "\n",
        "\n",
        "def _extract_events(text: str) -> list:\n",
        "    events = []\n",
        "    patterns = [\n",
        "        r'\\b(?:the\\s+)?([A-Z][a-zA-Z0-9\\s\\'\\-]+(?:Conference|Summit|Forum|Meeting|Festival|Games|Olympics|Championship|Cup|Awards|Exhibition|Show|Ceremony|Campaign|Initiative|Project|Program|Operation|War|Battle|Treaty|Accord|Act|Bill|Law|Debate|Election|Crisis|Pandemic|Outbreak|Attack|Incident|Disaster))\\b',\n",
        "        r'\\b([A-Z][a-zA-Z]+\\s+(?:World Cup|Olympic Games|Grand Prix|Open|Summit|Conference|Festival))\\b',\n",
        "        r'\\b(?:G7 Summit|G20 Summit|COP\\d+\\sConference)\\b',\n",
        "        r'\\b\\d{4}\\s+(?:Summer|Winter)\\s+Olympics\\b',\n",
        "    ]\n",
        "\n",
        "    known_events = ['World War I', 'World War II', 'Vietnam War', 'Cold War', 'September 11 attacks', 'COVID-19 Pandemic']\n",
        "\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "\n",
        "        events.extend([m if isinstance(m, str) else m[0] for m in matches])\n",
        "\n",
        "    for event in known_events:\n",
        "        if re.search(r'\\b' + re.escape(event) + r'\\b', text, re.IGNORECASE):\n",
        "            events.append(event)\n",
        "\n",
        "    processed_events = [event.strip().rstrip(',.') for event in events if len(event.strip()) > 4]\n",
        "    final_events = list(set(processed_events))\n",
        "    final_events.sort(key=len, reverse=True)\n",
        "    return final_events[:5]\n",
        "\n",
        "def _extract_dates(text: str, provided_date: str = None) -> list:\n",
        "    dates = []\n",
        "    if provided_date:\n",
        "        dates.append(provided_date.strip())\n",
        "\n",
        "    patterns = [\n",
        "\n",
        "        r'\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{1,2}(?:st|nd|rd|th)?,\\s+\\d{4}\\b',\n",
        "        r'\\b\\d{1,2}\\s+(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{4}\\b',\n",
        "\n",
        "        r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b',\n",
        "        r'\\b\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}\\b',\n",
        "\n",
        "        r'\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{4}\\b',\n",
        "\n",
        "        r'\\b(?:Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)(?:,\\s*(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{1,2}(?:st|nd|rd|th)?,\\s+\\d{4})?\\b',\n",
        "\n",
        "        r'\\b(?:yesterday|today|tomorrow|last\\s+week|next\\s+week|last\\s+month|next\\s+month|this\\s+year|last\\s+year|next\\s+year)\\b',\n",
        "\n",
        "        r'\\b(?:in|during|on|by|since|until|from|the\\s+year\\s+of)\\s+(\\d{4})\\b',\n",
        "        r'\\b(\\d{4})\\b'\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        if pattern.endswith(r'\\b(\\d{4})\\b') or pattern.endswith(r'\\s+(\\d{4})\\b') :\n",
        "            dates.extend(m for m in matches if 1900 <= int(m) <= 2050)\n",
        "        else:\n",
        "            dates.extend(m.strip() for m in matches)\n",
        "\n",
        "\n",
        "\n",
        "    final_dates = []\n",
        "    current_year = 2025\n",
        "    for d_match in dates:\n",
        "        d = d_match.strip().rstrip(',.')\n",
        "        if d.isdigit() and len(d) == 4:\n",
        "            year = int(d)\n",
        "            if 1900 <= year <= current_year + 5:\n",
        "                final_dates.append(d)\n",
        "        elif len(d) > 3:\n",
        "            final_dates.append(d)\n",
        "\n",
        "    final_dates = list(set(final_dates))\n",
        "\n",
        "    final_dates.sort(key=lambda x: (len(x), x), reverse=True)\n",
        "    return final_dates[:5]\n",
        "\n",
        "def _extract_key_terms(text: str, title:str) -> list:\n",
        "    combined_text = (title.lower() + \" \") * 3 + text.lower()\n",
        "\n",
        "\n",
        "    text_no_urls = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+', '', combined_text)\n",
        "\n",
        "\n",
        "    words = re.findall(r'\\b[a-zA-Z0-9][a-zA-Z0-9\\-\\']*[a-zA-Z0-9]\\b', text_no_urls)\n",
        "\n",
        "\n",
        "    stop_words = {\n",
        "        'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'being', 'been', 'this', 'that', 'these', 'those',\n",
        "        'and', 'or', 'but', 'if', 'of', 'at', 'by', 'for', 'with', 'about', 'to', 'from', 'in', 'out', 'on',\n",
        "        'it', 'its', 'he', 'she', 'they', 'them', 'his', 'her', 'their', 'you', 'your', 'we', 'our',\n",
        "        'i', 'me', 'my', 'mine', 'us', 'ours', 'myself', 'yourself', 'himself', 'herself', 'itself', 'ourselves', 'yourselves', 'themselves',\n",
        "        'what', 'which', 'who', 'whom', 'whose', 'why', 'how', 'when', 'where',\n",
        "        'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
        "        'can', 'will', 'just', 'don', 'should', 'now', 'do', 'does', 'did', 'doing', 'said', 'says', 'also', 'get', 'go', 'make', 'know', 'see', 'use', 'find', 'tell', 'ask', 'work', 'seem', 'feel', 'try', 'leave', 'call',\n",
        "        'one', 'two', 'three', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december',\n",
        "        'mr', 'mrs', 'ms', 'dr', 'prof', 'inc', 'ltd', 'corp',\n",
        "        'news', 'report', 'story', 'article', 'image', 'photo', 'picture', 'video', 'caption', 'description',\n",
        "        'people', 'person', 'man', 'woman', 'child', 'children', 'group', 'team',\n",
        "        'world', 'country', 'city', 'government', 'company', 'organization', 'event', 'system', 'part', 'number', 'way', 'thing', 'day', 'year', 'time', 'today', 'content', 'information', 'context', 'detail', 'example'\n",
        "    }\n",
        "\n",
        "\n",
        "    proper_nouns_phrases = re.findall(r'\\b[A-Z][a-zA-Z0-9\\-\\']*(?:\\s+[A-Z][a-zA-Z0-9\\-\\']*){0,3}\\b', text)\n",
        "    filtered_proper_nouns = []\n",
        "    for phrase in proper_nouns_phrases:\n",
        "        p_words = phrase.split()\n",
        "        if not all(word.lower() in stop_words for word in p_words) and \\\n",
        "           not (len(p_words) == 1 and p_words[0].lower() in stop_words) and \\\n",
        "           len(phrase.strip()) > 2 :\n",
        "            filtered_proper_nouns.append(phrase.strip())\n",
        "\n",
        "\n",
        "    filtered_words = [word for word in words if word not in stop_words and len(word) > 2 and not word.isdigit()]\n",
        "\n",
        "    term_freq = {}\n",
        "    for term in filtered_proper_nouns + title.lower().split():\n",
        "        if term.lower() not in stop_words and len(term)>2:\n",
        "            term_freq[term.lower()] = term_freq.get(term.lower(), 0) + 2\n",
        "    for word in filtered_words:\n",
        "        term_freq[word] = term_freq.get(word, 0) + 1\n",
        "\n",
        "\n",
        "    sorted_terms = sorted(term_freq.items(), key=lambda x: (x[1], len(x[0].split()), len(x[0])), reverse=True)\n",
        "\n",
        "    final_terms = []\n",
        "    seen_lower = set()\n",
        "    for term, freq in sorted_terms:\n",
        "        if term not in seen_lower:\n",
        "            original_case_term = term\n",
        "            for pn in filtered_proper_nouns:\n",
        "                if pn.lower() == term:\n",
        "                    original_case_term = pn\n",
        "                    break\n",
        "            final_terms.append(original_case_term)\n",
        "            seen_lower.add(term)\n",
        "        if len(final_terms) >= 10:\n",
        "            break\n",
        "\n",
        "    return final_terms\n",
        "\n",
        "\n",
        "def _extract_numbers(text: str) -> list:\n",
        "\n",
        "    patterns = [\n",
        "\n",
        "        r'(?:\\$|€|£|¥|USD|EUR|GBP|JPY|VND)\\s*\\d+(?:[.,]\\d{3})*(?:[.,]\\d+)?(?:\\s*(?:million|billion|trillion|thousand|K|M|B|T))?\\b',\n",
        "        r'\\b\\d+(?:[.,]\\d{3})*(?:[.,]\\d+)?\\s*(?:dollars?|euros?|pounds?|yen|đồng|USD|EUR|GBP|JPY|VND)(?:\\s*(?:million|billion|trillion|thousand|K|M|B|T))?\\b',\n",
        "\n",
        "        r'\\b\\d+(?:[.,]\\d+)?\\s*%(?:\\s*points)?\\b',\n",
        "        r'\\b\\d+(?:[.,]\\d+)?\\s*(?:percent|per\\s+cent|percentage\\s+points?)\\b',\n",
        "\n",
        "        r'\\b\\d+(?:[.,]\\d{3})*(?:[.,]\\d+)?\\s*(?:people|users|viewers|votes|cases|deaths|infections|jobs|companies|countries|cities|members|students|teachers|schools|hospitals|doctors|patients|items|products|services|cars|houses|buildings|acres|hectares|tons|kg|grams|liters|gallons|km|kilometers|meters|miles|feet|gb|mb|tb|hz|watts|volts|degrees|°C|°F|points|barrels|shares|pages|chapters|articles|sections|votes)\\b',\n",
        "\n",
        "        r'\\b\\d+(?:[.,]\\d+)?\\s*(?:to|-|–)\\s*\\d+(?:[.,]\\d+)?\\b',\n",
        "\n",
        "        r'\\b\\d{1,3}(?:[.,]\\d{3})*(?:[.,]\\d+)?\\b',\n",
        "        r'\\b(?:age|aged)\\s+\\d+\\b',\n",
        "        r'\\b\\d+\\s*years?\\s*old\\b',\n",
        "    ]\n",
        "    numbers = []\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        numbers.extend(matches)\n",
        "\n",
        "\n",
        "\n",
        "    processed_numbers = []\n",
        "    for num_match in numbers:\n",
        "        num_str = num_match.strip()\n",
        "        try:\n",
        "            if re.fullmatch(r'\\d+', num_str) and int(num_str) < 10:\n",
        "                if not any(unit in text[text.find(num_str):text.find(num_str)+len(num_str)+10].lower() for unit in ['million', 'billion', 'thousand', '%', 'percent', 'degree']):\n",
        "                    continue\n",
        "        except ValueError:\n",
        "            pass\n",
        "        processed_numbers.append(num_str)\n",
        "\n",
        "    final_numbers = list(set(processed_numbers))\n",
        "    final_numbers.sort(key=lambda x: any(kw in x.lower() for kw in ['million', 'billion', 'trillion', 'percent', '%', '$', '€', '£']), reverse=True)\n",
        "    return final_numbers[:10]\n",
        "\n",
        "\n",
        "def extract_key_info_from_json_item(item_content: str, item_title: str, item_date: str) -> dict:\n",
        "    if not item_content and not item_title:\n",
        "        return {\n",
        "            'titles': [], 'sources': [], 'topic': 'general', 'organizations': [],\n",
        "            'people': [], 'locations': [], 'events': [], 'numbers': [],\n",
        "            'dates': [item_date.strip()] if item_date else [], 'key_terms': [], 'context': ''\n",
        "        }\n",
        "\n",
        "    full_text_for_extraction = (item_title if item_title else \"\") + ' ' + (item_content if item_content else \"\")\n",
        "\n",
        "\n",
        "    extracted_title = [item_title.strip()] if item_title else []\n",
        "    extracted_dates = _extract_dates(full_text_for_extraction, item_date)\n",
        "    extracted_organizations = _extract_organizations(full_text_for_extraction)\n",
        "\n",
        "\n",
        "    temp_people = _extract_people(full_text_for_extraction)\n",
        "    extracted_people = [p for p in temp_people if p not in extracted_organizations and not any(org_part in p for org_part in \" \".join(extracted_organizations).split() if len(org_part)>3)]\n",
        "\n",
        "\n",
        "    temp_locations = _extract_locations(full_text_for_extraction)\n",
        "    extracted_locations = [l for l in temp_locations if l not in extracted_organizations and l not in extracted_people and not any(org_part in l for org_part in \" \".join(extracted_organizations).split() if len(org_part)>3)]\n",
        "\n",
        "\n",
        "    info = {\n",
        "        'titles': extracted_title,\n",
        "        'sources': [],\n",
        "        'topic': _identify_topic(full_text_for_extraction, extracted_title),\n",
        "        'organizations': extracted_organizations,\n",
        "        'people': extracted_people,\n",
        "        'locations': extracted_locations,\n",
        "        'events': _extract_events(full_text_for_extraction),\n",
        "        'numbers': _extract_numbers(full_text_for_extraction),\n",
        "        'dates': extracted_dates,\n",
        "        'key_terms': _extract_key_terms(full_text_for_extraction, item_title if item_title else \"\"),\n",
        "        'context': item_content if item_content else \"\"\n",
        "    }\n",
        "    return info"
      ],
      "metadata": {
        "id": "y5TR1mI3fSdE",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-06T03:34:29.463180Z",
          "iopub.execute_input": "2025-06-06T03:34:29.463459Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def _semantic_article_extraction(full_content: str, base_caption: str, key_info: dict) -> str:\n",
        "    global sentence_model_global\n",
        "    if not USE_SEMANTIC_SEARCH or sentence_model_global is None:\n",
        "        return full_content[:5000] + \"...\" if len(full_content) > 5000 else full_content\n",
        "\n",
        "    try:\n",
        "\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', full_content.strip())\n",
        "        sentences = [s.strip() for s in sentences if len(s.strip()) > 15]\n",
        "\n",
        "        if not sentences or len(sentences) < 5 :\n",
        "            return full_content[:5000] + \"...\" if len(full_content) > 5000 else full_content\n",
        "\n",
        "\n",
        "        chunk_size = 3\n",
        "        overlap = 1\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(0, len(sentences) - chunk_size + 1, chunk_size - overlap):\n",
        "            chunk = ' '.join(sentences[i : i + chunk_size])\n",
        "            if chunk.strip():\n",
        "                chunks.append(chunk)\n",
        "\n",
        "        if not chunks:\n",
        "            return full_content[:5000] + \"...\" if len(full_content) > 5000 else full_content\n",
        "\n",
        "\n",
        "        search_queries = [base_caption]\n",
        "        if key_info.get('titles'): search_queries.append(f\"Title context: {key_info['titles'][0]}\")\n",
        "        if key_info.get('key_terms'): search_queries.append(f\"Key terms: {', '.join(key_info['key_terms'][:3])}\")\n",
        "        if key_info.get('people'): search_queries.append(f\"People involved: {', '.join(key_info['people'][:2])}\")\n",
        "        if key_info.get('organizations'): search_queries.append(f\"Organizations: {', '.join(key_info['organizations'][:2])}\")\n",
        "        if key_info.get('locations'): search_queries.append(f\"Locations: {', '.join(key_info['locations'][:2])}\")\n",
        "\n",
        "\n",
        "        chunk_embeddings = sentence_model_global.encode(chunks, show_progress_bar=False, batch_size=128)\n",
        "        query_embeddings = sentence_model_global.encode(search_queries, show_progress_bar=False, batch_size=128)\n",
        "\n",
        "\n",
        "        all_sim_scores = []\n",
        "        for query_emb in query_embeddings:\n",
        "            sim_scores = np.dot(chunk_embeddings, query_emb) / (np.linalg.norm(chunk_embeddings, axis=1) * np.linalg.norm(query_emb))\n",
        "            all_sim_scores.append(sim_scores)\n",
        "\n",
        "\n",
        "        combined_scores = np.max(np.array(all_sim_scores), axis=0)\n",
        "\n",
        "\n",
        "        num_top_chunks = min(max(5, int(len(chunks) * 0.3)), 10)\n",
        "\n",
        "        top_indices_by_score = np.argsort(combined_scores)[-num_top_chunks:]\n",
        "\n",
        "\n",
        "        selected_chunks_with_scores = []\n",
        "        for idx in top_indices_by_score:\n",
        "            selected_chunks_with_scores.append((idx, chunks[idx], combined_scores[idx]))\n",
        "\n",
        "\n",
        "        selected_chunks_with_scores.sort(key=lambda x: x[0])\n",
        "\n",
        "\n",
        "        relevant_text_parts = [chunk_data[1] for chunk_data in selected_chunks_with_scores]\n",
        "\n",
        "\n",
        "        final_relevant_sentences = set()\n",
        "\n",
        "        for part in relevant_text_parts:\n",
        "            for s_in_chunk in re.split(r'(?<=[.!?])\\s+', part.strip()):\n",
        "                 if s_in_chunk.strip():\n",
        "                    final_relevant_sentences.add(s_in_chunk.strip())\n",
        "\n",
        "\n",
        "\n",
        "        first_sentences = [s.strip() for s in sentences[:min(3, len(sentences))] if s.strip()]\n",
        "        last_sentences = [s.strip() for s in sentences[max(0, len(sentences)-2):] if s.strip()]\n",
        "\n",
        "\n",
        "        combined_set = set(first_sentences) | final_relevant_sentences | set(last_sentences)\n",
        "\n",
        "\n",
        "\n",
        "        sentence_to_original_index = {sentence: i for i, sentence in enumerate(sentences)}\n",
        "\n",
        "        sorted_combined_sentences = sorted(list(combined_set), key=lambda s: sentence_to_original_index.get(s, float('inf')))\n",
        "\n",
        "\n",
        "        extracted_text = ' '.join(sorted_combined_sentences)\n",
        "\n",
        "\n",
        "        max_len = 6000\n",
        "        if len(extracted_text) > max_len:\n",
        "            extracted_text = extracted_text[:max_len]\n",
        "            last_sentence_end = extracted_text.rfind('.')\n",
        "            if last_sentence_end > 0:\n",
        "                extracted_text = extracted_text[:last_sentence_end+1]\n",
        "            else:\n",
        "                extracted_text += \"...\"\n",
        "        return extracted_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erron in semantic search: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return full_content[:5000] + \"...\" if len(full_content) > 5000 else full_content"
      ],
      "metadata": {
        "id": "D_XAd6Dm28oR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def create_advanced_prompt(base_caption: str, key_info: dict) -> str:\n",
        "    topic = key_info.get('topic', 'general')\n",
        "    source_str = \"\"\n",
        "\n",
        "    organizations = key_info.get('organizations', [])[:3]\n",
        "    people = key_info.get('people', [])[:3]\n",
        "    locations = key_info.get('locations', [])[:3]\n",
        "    events = key_info.get('events', [])[:2]\n",
        "    numbers = key_info.get('numbers', [])[:3]\n",
        "    dates = key_info.get('dates', [])[:3]\n",
        "\n",
        "    full_context = key_info.get('context', '')\n",
        "    article_title = key_info.get('titles', [''])[0]\n",
        "\n",
        "    context_elements = []\n",
        "    if article_title:\n",
        "        context_elements.append(f\"MAIN STORY: {article_title}\")\n",
        "    if organizations:\n",
        "        context_elements.append(f\"KEY ORGANIZATIONS: {', '.join(organizations)}\")\n",
        "    if people:\n",
        "        context_elements.append(f\"PEOPLE INVOLVED: {', '.join(people)}\")\n",
        "    if locations:\n",
        "        context_elements.append(f\"LOCATIONS: {', '.join(locations)}\")\n",
        "    if events:\n",
        "        context_elements.append(f\"EVENTS: {', '.join(events)}\")\n",
        "    if numbers:\n",
        "        context_elements.append(f\"KEY FIGURES: {', '.join(numbers)}\")\n",
        "    if dates:\n",
        "        context_elements.append(f\"TIMELINE: {', '.join(dates)}\")\n",
        "\n",
        "    article_summary = \"\"\n",
        "\n",
        "    if full_context:\n",
        "        if USE_SEMANTIC_SEARCH and sentence_model_global and len(full_context) > 3000:\n",
        "            article_summary = _semantic_article_extraction(full_context, base_caption, key_info)\n",
        "        else:\n",
        "            article_length = len(full_context)\n",
        "            if article_length <= 3000:\n",
        "                # Short article: use full content\n",
        "                article_summary = full_context\n",
        "            elif article_length <= 8000:\n",
        "                # Medium article: smart sampling with higher density\n",
        "                sentences = full_context.split('. ')\n",
        "                total_sentences = len(sentences)\n",
        "\n",
        "                # Take more sentences for better coverage\n",
        "                key_sentences = []\n",
        "\n",
        "                # First 10 sentences (usually most important)\n",
        "                key_sentences.extend(sentences[:10])\n",
        "\n",
        "                # Every 3rd sentence from the middle section\n",
        "                middle_start = 8\n",
        "                middle_end = total_sentences - 4\n",
        "                for i in range(middle_start, middle_end, 3):\n",
        "                  if i < total_sentences:\n",
        "                    key_sentences.append(sentences[i])\n",
        "\n",
        "                # Last 4 sentences (conclusions, outcomes)\n",
        "                key_sentences.extend(sentences[-4:])\n",
        "\n",
        "                # Remove duplicates while preserving order\n",
        "                seen = set()\n",
        "                unique_sentences = []\n",
        "                for sentence in key_sentences:\n",
        "                  if sentence.strip() and sentence not in seen:\n",
        "                    seen.add(sentence)\n",
        "                    unique_sentences.append(sentence)\n",
        "\n",
        "                article_summary = '. '.join(unique_sentences)\n",
        "            else:\n",
        "              article_summary = full_context[:5000]\n",
        "\n",
        "              # Add key sentences from the rest\n",
        "              remaining_content = full_context[5000:]\n",
        "              remaining_sentences = remaining_content.split('. ')\n",
        "\n",
        "              # Add every 5th sentence from remaining content\n",
        "              additional_sentences = []\n",
        "              for i in range(0, len(remaining_sentences), 5):\n",
        "                if len(additional_sentences) < 20:\n",
        "                  additional_sentences.append(remaining_sentences[i])\n",
        "\n",
        "              if additional_sentences:\n",
        "                article_summary += \". \" + '. '.join(additional_sentences)\n",
        "\n",
        "\n",
        "        max_length = 6000\n",
        "        if len(article_summary) > max_length:\n",
        "          truncated = article_summary[:max_length]\n",
        "          last_period = truncated.rfind('. ')\n",
        "          if last_period > max_length * 0.85:\n",
        "            article_summary = truncated[:last_period + 1]\n",
        "          else:\n",
        "            article_summary = truncated + \"...\"\n",
        "\n",
        "        context_elements.append(f\"COMPREHENSIVE ARTICLE CONTENT: {article_summary}\")\n",
        "\n",
        "    context_str = '\\n'.join(context_elements) if context_elements else \"General news context available.\"\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"You are a news caption expert. Your task is to write a news caption that PRIORITIZES the article content and news significance over visual description.\n",
        "\n",
        "BRIEF VISUAL: {base_caption}\n",
        "\n",
        "PRIORITY NEWS CONTEXT{source_str}:\n",
        "Topic: {topic.title()}\n",
        "{context_str}\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "1. The NEWS CONTEXT is MORE IMPORTANT than visual details\n",
        "2. Start with \"The image shows\" but immediately connect to the news story\n",
        "3. Use 70% article information + 30% visual description (ensure the connectivity between each visual element and the article content)\n",
        "4. Focus on WHO, WHAT, WHY, WHEN, WHERE from the article\n",
        "5. Mention specific names, organizations, events from the article\n",
        "6. Explain the news significance and broader implications\n",
        "7. Only describe visual elements that support the news story\n",
        "8. Write 300-350 words prioritizing factual news content\n",
        "\n",
        "GOOD EXAMPLE (prioritizing news over visuals):\n",
        "\"The image shows the scene from a significant political development as President Biden announces new healthcare legislation during a White House ceremony. This landmark bill, supported by Democratic leadership including Speaker Pelosi, aims to expand Medicare coverage to millions of Americans. The legislation comes after months of negotiations with pharmaceutical companies and represents a major victory for the administration's domestic agenda. The outdoor ceremony, attended by healthcare advocates and congressional leaders, marks the culmination of a campaign promise made during the 2020 election. The new law is expected to reduce prescription drug costs by 15% and provide coverage for dental and vision services, affecting approximately 12 million seniors nationwide.\"\n",
        "\n",
        "YOUR CAPTION (prioritize article news content over visual description):\"\"\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "95zmxqfwxe7r"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "def create_sample_dataset():\n",
        "    \"\"\"Tạo file dataset mẫu nếu không tồn tại\"\"\"\n",
        "    sample_data = [\n",
        "        {\n",
        "            \"image_id\": \"sample_img_001\",\n",
        "            \"base_caption\": \"Several politicians are seated at a long table during a formal meeting in a well-lit conference room.\",\n",
        "            \"title\": \"International Summit Addresses Global Economic Challenges\",\n",
        "            \"content\": \"Leaders from twenty major economies convened today in Geneva for the annual Global Economic Forum (GEF). The summit, running from June 3rd to June 5th, 2025, aims to tackle pressing issues such as rising inflation, supply chain disruptions, and the future of digital currencies. Keynote speaker Dr. Aris Thorne, head of the World Monetary Institute (WMI), presented a stark outlook, urging coordinated international action. Discussions also involved representatives from major corporations like OmniCorp and TechSolutions Inc. A significant portion of the agenda is dedicated to sustainable development goals. The GEF's final communiqué is expected to outline a roadmap for global recovery. Last year's summit resulted in a $100 billion pledge for infrastructure projects.\",\n",
        "            \"date\": \"2025-06-03\",\n",
        "            \"label\": \"The image captures a moment from the Global Economic Forum in Geneva, where leaders from twenty nations gathered between June 3-5, 2025, to discuss global inflation and supply chain issues. Dr. Aris Thorne of the World Monetary Institute highlighted the need for coordinated action. The summit, also attended by corporations like OmniCorp, is focusing on sustainable development and digital currencies, with a final roadmap for economic recovery anticipated.\"\n",
        "        },\n",
        "        {\n",
        "            \"image_id\": \"sample_img_002\",\n",
        "            \"base_caption\": \"A scientist in a lab coat is looking intently at a test tube with blue liquid.\",\n",
        "            \"title\": \"Breakthrough in Cancer Research Announced by MedSynth Labs\",\n",
        "            \"content\": \"MedSynth Labs, a leading biomedical research institute based in Cambridge, today announced a significant breakthrough in the development of a novel targeted therapy for lung cancer. The research, published in the 'Journal of Oncology', details a new compound, LX-7, that has shown remarkable efficacy in preclinical trials, shrinking tumors by up to 80%. The research team, led by Dr. Lena Hanson, has been working on this project for over seven years. \\\"This could revolutionize how we treat certain aggressive forms of lung cancer,\\\" Dr. Hanson stated at a press conference. MedSynth Labs plans to begin Phase 1 human trials by early 2026. The study was partially funded by a $5 million grant from the National Health Foundation (NHF). This development offers new hope for patients worldwide.\",\n",
        "            \"date\": \"2025-06-04\",\n",
        "            \"label\": \"The image likely depicts a scientist at MedSynth Labs in Cambridge, a facility that recently announced a breakthrough in lung cancer research with a new compound, LX-7. Led by Dr. Lena Hanson and published in the 'Journal of Oncology' on June 4, 2025, the therapy showed an 80% tumor reduction in preclinical trials. MedSynth Labs, supported by a $5 million NHF grant, aims for human trials by early 2026, offering new hope for cancer treatment.\"\n",
        "        }\n",
        "    ]\n",
        "    with open(DATASET_PATH, 'w', encoding='utf-8') as f_json:\n",
        "        json.dump(sample_data, f_json, indent=4)\n",
        "\n",
        "def load_dataset():\n",
        "    if not os.path.exists(DATASET_PATH):\n",
        "        create_sample_dataset()\n",
        "    else:\n",
        "        print(f\"Found file dataset at: {DATASET_PATH}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
        "            dataset_json = json.load(f)\n",
        "        return dataset_json\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error when decoding file JSON: {e}\")\n",
        "        raise\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ File '{DATASET_PATH}' not exists.\")\n",
        "        raise\n",
        "\n",
        "def process_dataset_items(dataset_json):\n",
        "    all_conversations = []\n",
        "\n",
        "    for i, item in enumerate(tqdm(dataset_json, desc=\"processing items\", unit=\"item\")):\n",
        "        image_id = item.get('image_id', f'unknown_id_{i}')\n",
        "        base_caption = item.get(\"base_caption\", \"\").strip()\n",
        "        title = item.get(\"title\", \"\").strip()\n",
        "        content = item.get(\"content\", \"\").strip()\n",
        "        date = item.get(\"date\", \"\").strip()\n",
        "        label = item.get(\"label\", \"\").strip()\n",
        "\n",
        "        if not label:\n",
        "            continue\n",
        "        if not base_caption:\n",
        "            continue\n",
        "        if not title and not content:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            key_info = extract_key_info_from_json_item(content, title, date)\n",
        "            user_prompt = create_advanced_prompt(base_caption, key_info)\n",
        "\n",
        "            all_conversations.append([\n",
        "                {\"role\": \"user\", \"content\": user_prompt},\n",
        "                {\"role\": \"assistant\", \"content\": label},\n",
        "            ])\n",
        "        except Exception as e:\n",
        "            print(f\"Error when processing item {image_id}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            print(f\"Data of item is error: base_caption='{base_caption[:50]}...', title='{title[:50]}...'\")\n",
        "            continue\n",
        "\n",
        "    if not all_conversations:\n",
        "        raise ValueError(\"There is no conversation.\")\n",
        "\n",
        "    print(f\"Successfully process {len(all_conversations)} conversation examples from dataset.\")\n",
        "    return all_conversations\n",
        "\n",
        "def apply_chat_template(all_conversations, tokenizer):\n",
        "    if tokenizer.chat_template is None:\n",
        "        print(\"⚠️ Tokenizer do not have available template chat\")\n",
        "        tokenizer.chat_template = (\n",
        "            \"{% if messages[0]['role'] == 'system' %}\"\n",
        "            \"{% set loop_messages = messages[1:] %}\"\n",
        "            \"{% set system_message = messages[0]['content'] %}\"\n",
        "            \"{% else %}\"\n",
        "            \"{% set loop_messages = messages %}\"\n",
        "            \"{% set system_message = false %}\"\n",
        "            \"{% endif %}\"\n",
        "            \"{% for message in loop_messages %}\"\n",
        "            \"{% if loop.index0 == 0 %}<|begin_of_sentence|>{% endif %}\"\n",
        "            \"{% if message['role'] == 'user' %}\"\n",
        "            \"<|User|>{{ message['content'] }}\"\n",
        "            \"{% elif message['role'] == 'assistant' %}\"\n",
        "            \"<|Assistant|>{{ message['content'] }}\"\n",
        "            \"{% if loop.last %}<|end_of_sentence|>{% endif %}\"\n",
        "            \"{% endif %}\"\n",
        "            \"{% endfor %}\"\n",
        "        )\n",
        "\n",
        "    formatted_texts = tokenizer.apply_chat_template(\n",
        "        all_conversations,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False,\n",
        "    )\n",
        "\n",
        "    if formatted_texts:\n",
        "        print(formatted_texts[0][:1000] + \"...\")\n",
        "    else:\n",
        "        print(\"No data after applying template chat\")\n",
        "        raise ValueError(\"formatted_texts is empty. Check data processing and chat template application.\")\n",
        "\n",
        "    return formatted_texts\n",
        "\n",
        "def create_huggingface_dataset(formatted_texts):\n",
        "    df = pd.DataFrame({\"text\": formatted_texts})\n",
        "    combined_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "    if len(combined_dataset) > 0:\n",
        "        combined_dataset = combined_dataset.shuffle(seed=3407)\n",
        "        print(combined_dataset[0]['text'][:500] + \"...\")\n",
        "\n",
        "        combined_dataset.set_format(type=\"torch\")\n",
        "\n",
        "        return combined_dataset\n",
        "    else:\n",
        "        print(\"Dataset is empty after processing.\")\n",
        "        raise ValueError(\"Cannot proceed with an empty dataset.\")\n",
        "\n",
        "def prepare_training_data(tokenizer):\n",
        "\n",
        "    dataset_json = load_dataset()\n",
        "\n",
        "\n",
        "    all_conversations = process_dataset_items(dataset_json)\n",
        "\n",
        "\n",
        "    formatted_texts = apply_chat_template(all_conversations, tokenizer)\n",
        "\n",
        "\n",
        "    combined_dataset = create_huggingface_dataset(formatted_texts)\n",
        "\n",
        "    return combined_dataset"
      ],
      "metadata": {
        "id": "BLOgdBkeLMwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_CONFIG[\"model_name\"],\n",
        "    max_seq_length=MODEL_CONFIG[\"max_seq_length\"],\n",
        "    load_in_4bit=MODEL_CONFIG[\"load_in_4bit\"],\n",
        "    load_in_8bit=MODEL_CONFIG[\"load_in_8bit\"],\n",
        "    full_finetuning=MODEL_CONFIG[\"full_finetuning\"],\n",
        ")"
      ],
      "metadata": {
        "id": "QmUBVEnvCDJv",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-06T03:33:26.176947Z",
          "iopub.execute_input": "2025-06-06T03:33:26.177275Z",
          "iopub.status.idle": "2025-06-06T03:33:30.554914Z",
          "shell.execute_reply.started": "2025-06-06T03:33:26.177252Z",
          "shell.execute_reply": "2025-06-06T03:33:30.554292Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = LORA_CONFIG['r'],\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = LORA_CONFIG['lora_alpha'],\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-06T03:33:30.556009Z",
          "iopub.execute_input": "2025-06-06T03:33:30.556247Z",
          "iopub.status.idle": "2025-06-06T03:33:37.173801Z",
          "shell.execute_reply.started": "2025-06-06T03:33:30.556229Z",
          "shell.execute_reply": "2025-06-06T03:33:37.172922Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset = prepare_training_data(tokenizer)"
      ],
      "metadata": {
        "id": "euH_Rc_JNLb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "def setup_training_args():\n",
        "    training_args_sft = SFTConfig(\n",
        "        output_dir=TRAINING_CONFIG[\"output_dir\"],\n",
        "        dataset_text_field=TRAINING_CONFIG[\"dataset_text_field\"],\n",
        "        max_seq_length=TRAINING_CONFIG[\"max_seq_length\"],\n",
        "        per_device_train_batch_size=TRAINING_CONFIG[\"per_device_train_batch_size\"],\n",
        "        gradient_accumulation_steps=TRAINING_CONFIG[\"gradient_accumulation_steps\"],\n",
        "        warmup_steps=TRAINING_CONFIG[\"warmup_steps\"],\n",
        "        num_train_epochs=TRAINING_CONFIG[\"num_train_epochs\"],\n",
        "        max_steps=TRAINING_CONFIG[\"max_steps\"],\n",
        "        learning_rate=TRAINING_CONFIG[\"learning_rate\"],\n",
        "        logging_steps=TRAINING_CONFIG[\"logging_steps\"],\n",
        "        optim=TRAINING_CONFIG[\"optim\"],\n",
        "        weight_decay=TRAINING_CONFIG[\"weight_decay\"],\n",
        "        lr_scheduler_type=TRAINING_CONFIG[\"lr_scheduler_type\"],\n",
        "        seed=TRAINING_CONFIG[\"seed\"],\n",
        "        report_to=TRAINING_CONFIG[\"report_to\"],\n",
        "        dataloader_num_workers=TRAINING_CONFIG[\"dataloader_num_workers\"],\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=10,\n",
        "    )\n",
        "\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        if torch.cuda.get_device_capability()[0] >= 8:\n",
        "            training_args_sft.bf16 = True\n",
        "            print(\"BF16 enabled for training.\")\n",
        "        else:  # Older GPUs\n",
        "            training_args_sft.fp16 = True\n",
        "            print(\"FP16 enabled for training.\")\n",
        "\n",
        "\n",
        "        training_args_sft.gradient_checkpointing = True\n",
        "        print(\"Gradient checkpointing enabled.\")\n",
        "\n",
        "    return training_args_sft\n",
        "\n",
        "def print_gpu_stats():\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_stats = torch.cuda.get_device_properties(0)\n",
        "        start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024**3, 3)\n",
        "        max_memory = round(gpu_stats.total_memory / 1024**3, 3)\n",
        "        print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "        print(f\"{start_gpu_memory} GB of memory reserved before training.\")\n",
        "        return start_gpu_memory, max_memory\n",
        "    else:\n",
        "        print(\"CUDA is not available\")\n",
        "        return 0, 0\n",
        "\n",
        "def print_training_stats(trainer_stats, start_gpu_memory, max_memory):\n",
        "    if torch.cuda.is_available():\n",
        "        used_memory = round(torch.cuda.max_memory_reserved() / 1024**3, 3)\n",
        "        used_memory_for_training = round(used_memory - start_gpu_memory, 3)\n",
        "        used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "        training_percentage = round(used_memory_for_training / max_memory * 100, 3)\n",
        "        print(f\"\\n{trainer_stats.metrics['train_runtime']:.2f} seconds used for training.\")\n",
        "        print(f\"{trainer_stats.metrics['train_runtime']/60:.2f} minutes used for training.\")\n",
        "        print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "        print(f\"Peak reserved memory for training = {used_memory_for_training} GB.\")\n",
        "        print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "        print(f\"Peak reserved memory for training % of max memory = {training_percentage} %.\")\n",
        "    else:  # CPU training\n",
        "        print(f\"\\n{trainer_stats.metrics['train_runtime']:.2f} seconds used for training (CPU).\")\n",
        "\n",
        "def save_model_and_tokenizer(trainer, tokenizer, training_args_sft):\n",
        "    final_model_path = os.path.join(training_args_sft.output_dir, \"final_checkpoint\")\n",
        "    trainer.save_model(final_model_path)\n",
        "    print(f\"Model/Adapter is saved at: {final_model_path}\")\n",
        "\n",
        "\n",
        "    if hasattr(trainer.model, 'peft_config'):\n",
        "        tokenizer.save_pretrained(final_model_path)\n",
        "        print(f\"Tokenizer is saved at: {final_model_path}\")\n",
        "\n",
        "def train_model(model, tokenizer, combined_dataset):\n",
        "    if not torch.cuda.is_available() or len(combined_dataset) == 0:\n",
        "        print(\"SFTTrainer is skipped due to the lack of CUDA or empty dataset.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    import os\n",
        "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "    # Setup training arguments\n",
        "    training_args_sft = setup_training_args()\n",
        "    # Force single process for dataset processing\n",
        "    training_args_sft.dataset_num_proc = 1\n",
        "\n",
        "    # Print GPU stats\n",
        "    start_gpu_memory, max_memory = print_gpu_stats()\n",
        "\n",
        "\n",
        "    import datasets\n",
        "    import multiprocessing as mp\n",
        "\n",
        "    # Force disable multiprocessing in datasets\n",
        "    datasets.disable_caching()\n",
        "\n",
        "    # Override multiprocessing functions to return 1\n",
        "    original_cpu_count = mp.cpu_count\n",
        "    def mock_cpu_count():\n",
        "        return 1\n",
        "    mp.cpu_count = mock_cpu_count\n",
        "\n",
        "    # Patch datasets to use only 1 process\n",
        "    if hasattr(datasets.config, 'DEFAULT_MAX_BATCH_SIZE'):\n",
        "        datasets.config.DEFAULT_MAX_BATCH_SIZE = 1000\n",
        "\n",
        "    # Force environment variable again\n",
        "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "    # Create trainer with explicit single-process settings\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=combined_dataset,\n",
        "        args=training_args_sft,\n",
        "        dataloader_num_workers=4,\n",
        "        packing=False,\n",
        "    )\n",
        "\n",
        "    # Restore original function\n",
        "    mp.cpu_count = original_cpu_count\n",
        "\n",
        "    print(\"\\nStart training the model...\")\n",
        "\n",
        "    if len(trainer.train_dataset) == 0:\n",
        "        print(\"dataset is empty.\")\n",
        "        return None\n",
        "    else:\n",
        "        try:\n",
        "            trainer_stats = trainer.train()\n",
        "\n",
        "            # Print training statistics\n",
        "            print_training_stats(trainer_stats, start_gpu_memory, max_memory)\n",
        "\n",
        "            # Save model and tokenizer\n",
        "            save_model_and_tokenizer(trainer, tokenizer, training_args_sft)\n",
        "\n",
        "            return trainer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in training: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "def save_final_model(model, tokenizer, save_path=\"/content/drive/MyDrive/lora_model_512\"):\n",
        "    print(f\"Saving the final model at: {save_path}\")\n",
        "    model.save_pretrained(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "    print(f\"Successfully save model at: {save_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vbpvq7e4QJFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = train_model(model, tokenizer, combined_dataset)\n",
        "print(\"save the final model\")\n",
        "save_final_model(model, tokenizer)"
      ],
      "metadata": {
        "id": "oH5mXin3Vkl2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}